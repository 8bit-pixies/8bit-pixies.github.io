[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Just a machine learning engineer who happens to build models and other stuff."
  },
  {
    "objectID": "posts/a-neural-probabilistic-model/index.html",
    "href": "posts/a-neural-probabilistic-model/index.html",
    "title": "Revisting A Neural Probabilistic Model",
    "section": "",
    "text": "The sketch code for neural probabilistic model using GPT style model (or anything from HuggingFace) - we should be able to calculate perplexity and do some kind of language modeling.\nneural_ngram_config = GPT2Config(\n    vocab_size=vocab_size, n_positions=max_name_length, n_layer=2, n_embd=256, n_inner=256, n_head=8\n)\nmodel = GPT2LMHeadModel(neural_ngram_config)\n\naccelerator = Accelerator()\ntraining_args = TrainingArguments(\n    output_dir=\"gibberish-sample-model\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    num_train_epochs=20,\n    fp16=False,\n)\n\ntrainer = accelerator.prepare(\n    Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=lm_dataset[\"train\"],\n        eval_dataset=lm_dataset[\"test\"],\n        data_collator=data_collator,\n    )\n)\n\ntrainer.train()\n\ncheckpoint_name = \"gibberish-gpt2-sample\"\nmodel.save_pretrained(checkpoint_name)\n\nfinal_model = AutoModelForCausalLM.from_pretrained(checkpoint_name)\n\n\ndef evaluate_model(name, final_model):\n    inputs = tokenizer(name, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = final_model(inputs['input_ids'], labels=inputs['input_ids'])\n    # Shift so that tokens &lt; n predict n\n    shift_logits = outputs.logits[..., :-1, :].contiguous()\n    shift_labels = inputs['input_ids'][..., 1:].contiguous()\n    # Flatten the tokens\n    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n    loss = torch.exp(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)))\n    return torch.mean(loss, dim=-1)\n\n\nevaluate_model(\"cheryl\".lower(), final_model)\n\n\n# wrap around a pytorch module\nclass GPT2PerplexityExport(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, input_ids):\n        batch_size = input_ids.size(0)\n        logits = self.model(input_ids, labels=input_ids)[1]\n        # Shift so that tokens &lt; n predict n\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = input_ids[..., 1:].contiguous()\n        # Flatten the tokens\n        loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        loss = torch.exp(loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))).reshape(batch_size, -1)\n        return torch.mean(loss, dim=-1)\n\n    def export(self):\n        onnx_bytes = io.BytesIO()\n        # output of the node above is the probablities calls \"output\"\n        torch.onnx.export(\n            self,\n            torch.ones(1, 2).long(),\n            onnx_bytes,\n            dynamic_axes={\"input_ids\": [0, 1], \"perplexity\": [0]},\n            input_names=[\"input_ids\"],\n            output_names=[\"perplexity\"],\n        )\n        onnx_model = onnx.load_model_from_string(onnx_bytes.getvalue())\n        return onnx_model\n    \ngpt2_perplexity = GPT2PerplexityExport(model = final_model).export()\n\nwith open(\"gibberish_gpt2_perplexity.onnx\", \"wb\") as f:\n    f.write(gpt2_perplexity.SerializeToString())"
  },
  {
    "objectID": "posts/gentle-boost/index.html",
    "href": "posts/gentle-boost/index.html",
    "title": "GentleBoost Algorithm",
    "section": "",
    "text": "Often in academic literature, boosting is introduced through AdaBoost or maybe sometimes they jump straight into Gradient Boosting. In this post, I thought I’ll introduce it in a different way, using “Gentle Boosting”, which is far easier to implement and understand.\nIn Python-esque code, Gentle Boost is implemented as follows:\n# with training set, &lt;X, y&gt;\nlearning_penalty = 1/1.1  # a parameter of interest\nw = np.array([1.0 for record in X.shape[0]])\nfor model in models:\n    model.fit(X, y, weights=w)\n    # update w based on error\n    # set record 1 if correctly predicted, -1 if incorrect\n    weight_update_factor = ((y == model.predict(X)) * 2) -1\n    w = w * np.power(learning_penalty, weight_update_factor)\n\n# inference\nnp.mean([model.predict(X) for model in models])\nWhat’s going on here?\n\nWe train the model based on weights of the observation (higher weights of previous model got the prediction wrong, lower weight if the previous model got it correct)\nAt each step we augment the weights by a fixed pre-known penalty, by alpha for correct predictions and 1/alpha for incorrect predictions.\nFor prediction, simply take the average\n\nThis works in the gentle boosting framework by using newton steps to update the weights of the model. By having fixed step sizes for updating the weights it ensures that the weights are bounded and outliers won’t over-weigh the model. The original formulation uses scaling in the form of alpha = exp(1), based on the expoential objective function. In the above formation the scaling is a parameter of the model. This can be interpretted as a modified objective variation to gentle boost instead.\nI think this variation is very simple to understand and implement whilst still being somewhat more effective than not using boosting at all.\nFor more information please see an implementation of Gentleboost and the corresponding writeup."
  },
  {
    "objectID": "posts/prefect-thoughts/index.html",
    "href": "posts/prefect-thoughts/index.html",
    "title": "Initial Thoughts on Prefect",
    "section": "",
    "text": "I’ve previously looked at Dagster and Prefect as alternatives to Airflow. Initially I dismissed Prefect - Dagster seemed to have more things “out of the box” and just an overall more complete package, but challenges with deploying Dagster in a setup on a Kubernetes cluster without Stateful Sets and restricted RBAC means its difficult to take a concept to Production in a non-prod environment for experimentation.\nOne thing that came as a pleasant surprise for prefect was how easy it was to grow out the deployment gradually. We would deploy a simple python application via:\nprefect server start\nThen deploy another simple python application via:\n# have `my_flow.serve(...)`\npython -m my_prefect_module\nAnd thats it! We have a flow in production which can operate on a schedule (or via a trigger) with a dashboard.\nNow if we want to separate out compute more, then, we probably need to start diving into the Helm chart, though for simple workflows this makes things much easy to construct PoC and begin pushing the boundaries with a team’s maturity and workflows."
  },
  {
    "objectID": "posts/evaluating-cdfs/index.html",
    "href": "posts/evaluating-cdfs/index.html",
    "title": "Evaluating CDFs using SymPy",
    "section": "",
    "text": "SymPy is pretty useful for symbolic things. One of the items which I was surprised by, was the ability to interrogate and get “closed form” equations (at least to the most simplified symbolic representation) of “unusual” distributions which may not have CDF implementations naturally available. You can then use these representations to evaluate them. For example the Irwin-Hall distribution can be addressed as follows:\nfrom sympy.stats import UniformSum, cdf\nfrom sympy import Symbol\n\nn = Symbol(\"n\", integer=True)\nuniform_sum = UniformSum(\"x\", n)\nx = Symbol(\"x\")\ncdf(uniform_sum)(x)\n# Piecewise((0, x &lt; 0), (Sum((-1)**_k*(-_k + x)**n*binomial(n, _k), (_k, 0, floor(x)))/factorial(n), n &gt;= x), (1, True))\nWe can evaluate a particular distribution as follows:\ncdf(UniformSum(\"x\", 2), evaluate=True)(1.5).doit()\n# 0.875\nOne could print out the CDF and rewrite it in some kind of closed form solution, but having something handy like this is quite amazing as there doesn’t appear to be many “common” packages that implement this kind of interface.\nYou might be wondering how did I end up trying to calculate this? This actually comes from figuring out the “true hit” from the statistics displayed by Fire Emblem 6 in their combat calculation. In this formulation, the provided “probability” (out of 100) is actually based on taking the average of 2 random rolls of a 100 face die. To calculate this, we would do:\nhit_rate = 70\nscaling_factor = 1/50  # 2/100\ncdf(UniformSum(\"x\", 2), evaluate=True)(hit_rate * scaling_factor).doit()\n# 0.82\n\nhit_rate = 40\ncdf(UniformSum(\"x\", 2), evaluate=True)(hit_rate * scaling_factor).doit()\n# 0.32\nYou can see in the game it is more lenient towards higher accuracy numbers and punishes numbers under 50% much more"
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html",
    "href": "posts/a-simple-feature-store/index.html",
    "title": "A Simple Feature Store",
    "section": "",
    "text": "Feast is winding down (or at least Tecton is moving its maintainership status), I thought its time to re-think about “simple” feature stores. This isn’t because I think Feast is going away or it will fade into obscurity, but rather rethink the patterns and how we manage production."
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html#scoping",
    "href": "posts/a-simple-feature-store/index.html#scoping",
    "title": "A Simple Feature Store",
    "section": "Scoping",
    "text": "Scoping\nMany feature platform solutions solve both the feature generation, online and offline retrieval of features. To simplify the problem space, what if we only want to deal with online retrieval and offline storage. In this case, we’re ignoring the offline retrieval portion of the problem altogether.\nAlthough this massively reduces the scope of the problem, it also massively simplifies the problem space."
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html#high-level-design",
    "href": "posts/a-simple-feature-store/index.html#high-level-design",
    "title": "A Simple Feature Store",
    "section": "High Level Design",
    "text": "High Level Design\nIf we are focussed predominantly on the online retrieval portion of the feature store, then we are executing against a Kappa-style architecture.\nBatch Style Feature Publishing:\n\nRun ETL job\nSeparate process to load into online store (e.g. Redis)\n\nStreaming Style Feature Publishing:\n\nRun streaming process\nLoad data into online store\nSeparate process to write data back to offline storage\n\nThe key component here, is deciding on the data model to manage these style of features.\nSome of the challenges in approaching things in this way is discerning between the different ways data is stored:\n\nas a slowly changing record (e.g. a customer’s profile information)\nas an event record (e.g. a customer purchasing an item)\n\nIf we simplify our problem and pretend all information arriving is an event then our problem space becomes even easier."
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html#data-model",
    "href": "posts/a-simple-feature-store/index.html#data-model",
    "title": "A Simple Feature Store",
    "section": "Data Model",
    "text": "Data Model\n\nData dominates. If you’ve chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming. - Rob Pike\n\nIn the data model a machine learning takes in denormalised data. It is generally unnested so that variety of machine learning libraries can make use of it. Having it unnested also simplifies the pattern, because then data from different sources with different entity keys can be combined at inference time in a naive fashion.\nThen the target offline data setup consists of two tables: - Feature Set (Latest) - Feature Set (History)\nWe can model this (via Python dataclass notation):\nclass FeatureSetHistory:\n    id: int\n    feature_set: Dict[str, FeatureValue]  # flatten this out in a table\n    create_timestamp: datetime\n\n\nclass FeatureSetLatest:\n    id: int\n    feature_set: Dict[str, FeatureValue]  # flatten this out in a table\nFrom an ETL perspective, both tables should be populated at the same time. The online setting will bulk unload the FeatureSetLatest information into the appropriate key-value store for online inference.\nThis is equivalent to modeling data as an EAVT format"
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html#online-query",
    "href": "posts/a-simple-feature-store/index.html#online-query",
    "title": "A Simple Feature Store",
    "section": "Online Query",
    "text": "Online Query\nFrom an online query perspective, we would need some metadata which contains all possible feature sets, their entity infromation. Then the query would be:\nfeatureset_a, featureset_b, featureset_c = get_feature_set_by_entity_info(entity_info)\nkey_value_store.mget([featureset_a, featureset_b, featureset_c])\nwhere featureset_a, featureset_b, featureset_c, is the appropriate key for the feature sets each keyed with the appropriate entity"
  },
  {
    "objectID": "posts/a-simple-feature-store/index.html#publishing-offline",
    "href": "posts/a-simple-feature-store/index.html#publishing-offline",
    "title": "A Simple Feature Store",
    "section": "Publishing offline",
    "text": "Publishing offline\nWorking backwards, now that we have a table of events we can unravel them to a more friendly data warehousing solution, we would have a continual ETL job which goes from EAVT to a SCD table, such as via a regular ETL process to populate the appropriate data warehouse.\nFrom a retrieval perspective, the data housing patterns should be used to manage the modeling workflows instead of natively using the EAVT structure.\nThis also makes it plausible to build an appropriate retrieval SDK inhouse - you only need to worry about SCD style tables for each feature set rather than lots of different types of modeling tables."
  },
  {
    "objectID": "posts/challenge-with-llms/index.html",
    "href": "posts/challenge-with-llms/index.html",
    "title": "LLMs and lack of Interoperability",
    "section": "",
    "text": "As I go through the journey of testing our different LLMs and the nuances, one thing that seems to have stood out is that “knowledge and experience” of LLMs are not really transferable.\nThis isn’t to do with the quality of different LLMs but rather things from prompt-templates, contraining outputs have different interfaces and different patterns.\nFor example:\n\nllama.cpp use grammars to constrain outputs when doing token sampling\nopenai have their own function calling approach which has a different format altogether\nUsing external libraries like guidance don’t really ensure things across different LLMs are consistent (or more specifically it does, but can sometimes just fail to create output altogether)\n\nMaybe there will be structures to generate these things more consistently (or the industry will settle on a standard) but right now you need to be aware of the nuances.\nExample:\nMistral Instruct - the template of mistral instruct uses [/INST] OpenAI - the template uses ChatML\nUsers seemingly need to “test” whether certain prompts are needed (or not) for LLMs which seems wasteful.\nLet’s see how the space evolves."
  },
  {
    "objectID": "posts/life-well-lived/index.html",
    "href": "posts/life-well-lived/index.html",
    "title": "A life well lived",
    "section": "",
    "text": "The 21st century is dominated by the concept of utility - that something is done for the grind, or for the purpose of some greater existence (Effective Altruism anyone?). It is often difficult to justify anything if it doesn’t generate value to someone or something. This goes from corporate planning (e.g. OKRs) and “agile” project management, to the “ad-ification” of content and recommendation systems in social media.\nPerhaps then, we should reframe how we perceive our own life, and the purpose thereof\n\nA life well lived is a series of personal obsessions shared without expectation of an audience - source\n\nWe may hear of T-shape employees or the generalist versus specialist mindset, but lets face it, could you really claim to be the same specialist from 10 years ago? How many PhD holders can maintain and hold onto their mental acuity when they were still a student (without consistent revision)?\nPerhaps a another very real lens related to this issue, is simply the fear of criticism - I know this is a very real fear that I have. Maybe I don’t aim as high or don’t design as thoroughly because I’m simply “scared” that someone will review and critic my work! In many ways academic research has helped desensitise me against this (in a good way). Criticism if valid, should be treated seriously, but criticism for criticism sake can be safely ignored and disregarded.\nAt the end of the day, this really boils down to asking the question of “why” are you doing something, not the result. And that is only something you, dear reader, can answer for you."
  },
  {
    "objectID": "posts/coloring-sprites/index.html",
    "href": "posts/coloring-sprites/index.html",
    "title": "Coloring Sprites",
    "section": "",
    "text": "When playing around with Spritemaps, one challenge, is how do we colorize sprites? One way of doing this is using ColorMapFilter. ColorMapFilter is a $ 5 $ array where the columns are:\n\nred\ngreen\nblue\nalpha\noffset\n\nand the rows are:\n\nred\ngreen\nblue\nalpha\n\nThe key parts of ColorMapFilter is that it can be used to “brighten” colors and “remove” colors. To start off with the identity matrix will do nothing to the image:\n[\n    1.0, 0.0, 0.0, 0.0, 0.0,\n    0.0, 1.0, 0.0, 0.0, 0.0,\n    0.0, 0.0, 1.0, 0.0, 0.0,\n    0.0, 0.0, 0.0, 1.0, 0.0\n]\nFor example, if we want to brighten up the red then we can use:\n[\n    2.0, 0.0, 0.0, 0.0, 0.0,\n    0.0, 1.0, 0.0, 0.0, 0.0,\n    0.0, 0.0, 1.0, 0.0, 0.0,\n    0.0, 0.0, 0.0, 1.0, 0.0\n]\nwhich will increase the intensity of the red pixels, conversely if we want to reduce red\n[\n    0.5, 0.0, 0.0, 0.0, 0.0,\n    0.0, 1.0, 0.0, 0.0, 0.0,\n    0.0, 0.0, 1.0, 0.0, 0.0,\n    0.0, 0.0, 0.0, 1.0, 0.0\n]\nIn the Sprite above, we iterate through a set of filters to colorize the sprites in different ways. In this manner we can reuse the same sprite and generate sprites for different “team colors”."
  },
  {
    "objectID": "posts/dice-less-table-top-rpg-system/index.html",
    "href": "posts/dice-less-table-top-rpg-system/index.html",
    "title": "Diceless Table Top RPG Systems",
    "section": "",
    "text": "It is kind of weird to talk about table top gaming on a blog that is focussed more on AI and machine learning, but there are some important concepts related to thinking about the design of LLMs and story telling. Diceless and GM-less (GM is short for game-master) are interesting concepts as they remove a level of randomness and focus more on the storytelling aspect behind table top role-playing. There are games like Wanderhome, Belonging Outside Belonging, and ORBITAL fit this kind of criteria (there are more but I wanted to focus on token-based dice-less GM-less TTRPGs).\nHow do these systems work? From a high level perspective, a character have three types of actions:\nIn these systems, characters typically start with 0 tokens and gain takens when they perform a weak action, whilst lose tokens when they perform a strong action. To prevent characters from “hoarding” tokens, the system may either cap the maximum number of tokens or provide a discount/force characters to use their tokens. These systems may even be used in games with dice, such as “momentum” in Ironsworn"
  },
  {
    "objectID": "posts/dice-less-table-top-rpg-system/index.html#so-how-is-this-related-to-llms",
    "href": "posts/dice-less-table-top-rpg-system/index.html#so-how-is-this-related-to-llms",
    "title": "Diceless Table Top RPG Systems",
    "section": "So How Is This Related to LLMs?",
    "text": "So How Is This Related to LLMs?\nWhen thinking about LLM generation and how to use prompts to forward a story it, this system can be very powerful from being able to select actions from a character in a balanced manner. If a character approaches the story too positively or too negatively, the token system will allow the character to realign itself.\nThis level of interactivity can enable an interface for “choose your own adventure” to be in the format:\n&lt;Description of Event&gt;\n\nWhat do you do?\n1. &lt;Strong Action (if available)&gt;\n2. &lt;Basic Action&gt;\n3. &lt;Weak Action (if available)&gt;\n&gt;&gt;&gt; &lt;free text&gt;\nThen the free text can be one of the pre-canned actions or something which the LLM will infer. If the action is not possible, as it is deemed to be a strong or weak action then it will re-prompt the user.\nUsing this question and answer format can help propell a story and add randomness and shape a story in different directions. At a later stage, I will probably try writing stories using LLMs in an automated way using this kind of structured approach.\nThe advantages of doing it in this manner is that it is simple and we do not need to re-implement the full ruleset of a dungeons and dragons setup, we can easily keep track of state and generate text in the “typical” manner."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Maybe its time to “restart” my blog. I’ve actually been blogging for quite a while previously - though it was more adhoc.\nThis blog will place a heightened focus more on engineering and technology - the doing of things.\nHow did I build this blog?\nI decided to use Quarto to build this static blog. It provides a way to combine code outputs in a blog format. Perhaps we want to post research results or outcomes! This is a suitable way to do that."
  },
  {
    "objectID": "posts/suno-bark/index.html",
    "href": "posts/suno-bark/index.html",
    "title": "Generate Podcasts from a Script using Suno Bark!",
    "section": "",
    "text": "Today we’re taking a look at a text to speech library called Suno. Suno can generate realistic audio from a script.\nTo get started, simply install it in the normal way using - pip install as per the git repository instructions.\npip install git+https://github.com/suno-ai/bark.git\nThen to generate the audio, set the voice and download the appropriate weights.\nBy default, Suno only generates sentences that are at most fourteen seconds long. To make it longer, there are some tricks we employ by splitting it up into sentences and artificially adding some silence between the sentences.\nOne way to achieve this is to first split the sentences using nltk, though if the sentences are too long, Suno/Bark has a tendency to unnecessarily elongate the audio.\nYou can see how the script is written and the code to reproduce this audio file in its entirety. In the future I may use this to generate podcasts off written scripts and blog posts automatically!\nimport re\n\nimport nltk\nimport numpy as np\nimport scipy\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoProcessor\n\n\nclass TextToSpeech:\n    _processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n    _model = AutoModel.from_pretrained(\"suno/bark-small\")\n    _sampling_rate = _model.generation_config.sample_rate\n    _silence_in_seconds = 0.25\n\n    @classmethod\n    def _generate_from_sentence(cls, sent):\n        voice_preset = \"v2/en_speaker_9\"\n        inputs = cls._processor(\n            text=[sent],\n            return_tensors=\"pt\",\n            voice_preset=voice_preset,\n        )\n        speech_values = cls._model.generate(**inputs)\n        return speech_values.cpu().numpy().squeeze()\n\n    @classmethod\n    def _export_to_audio(cls, data_array, fname):\n        scipy.io.wavfile.write(fname, rate=cls._sampling_rate, data=data_array)\n\n    @classmethod\n    def generate_audio_from_long_script(cls, script, fname):\n        sentences = nltk.sent_tokenize(re.sub(r\"\\s+\", \" \", script))\n\n        silence = np.zeros(int(cls._silence_in_seconds * cls._sampling_rate))  # quarter second of silence\n\n        pieces = []\n        for sentence in tqdm(sentences):\n            audio_array = cls._generate_from_sentence(sentence)\n            pieces += [audio_array, silence.copy()]\n        data_array = np.concatenate(pieces)\n        cls._export_to_audio(data_array, fname)\n\n\nif __name__ == \"__main__\":\n\n    script = \"\"\"\n    Welcome - today we're taking a look at a text to speech library called Suno. \n    Suno can generate realistic audio from a script.\n\n    To get started, simply install it in the normal way using - \n    pip install as per the git repository instructions.\n    Then to generate the audio, set the voice and download the appropriate weights.\n    The voice that you're currently listening to is english speak number nine.\n\n    By default, Suno only generates sentences that are at most fourteen seconds long.\n    To make it longer, there are some tricks we employ by splitting it up into sentences\n    and artificially adding some silence between the sentences.\n\n    You can see how the script is written and the code to reproduce this audio file in its entirety.\n    In the future I may use this to generate podcasts off written scripts and blog posts automatically!\n    \"\"\"\n\n    TextToSpeech.generate_audio_from_long_script(script, \"podcast.wav\")\nListen to the audio here!\n\n\nYour browser does not support the audio element."
  },
  {
    "objectID": "posts/natural-language-ui/index.html",
    "href": "posts/natural-language-ui/index.html",
    "title": "Building Natural Language UI using Extract Question Answering",
    "section": "",
    "text": "I’ve been interested in how to build “safe” natural language interfaces in the last little while, and have been wracking my brain around how to do this in a probabilistic and safe manner.\nAlthough I don’t have everything solve at this point in time, I have at least figured out a system that will at least be:\n\nbounded\ncan run in memory on modest to perhaps low-end hardware\ncan be used to generate structured output\n\nAutomation Attempt\nMy initial automation attempt was to use Question Answering models like: https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad and then crafting generic prompts.\nThis actually works fairly well, but can fail spectacularly. It would look something like this:\n@dataclass\nclass FunctionArguments:\n    argument: str\n    another_parameter: str\n\n# generate questions\nargument_value_question = \"What is the value 'argument'?\"\nanother_parameter_value_question = \"What is the value of 'another parameter'?\"\ncontext = \"Execute my function where argument is hello world and my other parameter is foo_bar\"\nYou can try running this in Huggingface UI and you’ll get:\n{\n  \"score\": 0.9491994380950928,\n  \"start\": 38,\n  \"end\": 49,\n  \"answer\": \"hello world\"\n}\n{\n  \"score\": 0.9905983209609985,\n  \"start\": 76,\n  \"end\": 83,\n  \"answer\": \"foo_bar\"\n}\nwhich are quite impressive results, with probabilities about 0.9\nThis may get a bit harder without prompt engineering when the questions are a bit more abstract.\nFor example, if the fields are:\n\nsummary_length\ntopic\n\nWith a question “What is the value of summary length?”, with context “Write me a summary in 5 sentences or less about rainbows”\n{\n  \"score\": 0.30676600337028503,\n  \"start\": 22,\n  \"end\": 33,\n  \"answer\": \"5 sentences\"\n}\nAnd with question “What is the value of topic?”, with context “Write me a summary in 5 sentences or less about rainbows”\n{\n  \"score\": 0.4378773868083954,\n  \"start\": 48,\n  \"end\": 56,\n  \"answer\": \"rainbows\"\n}\nHowever notice with better prompts, if we change the question to “What is the topic we are writing about?”, with context “Write me a summary in 5 sentences or less about rainbows”\n{\n  \"score\": 0.9759015440940857,\n  \"start\": 48,\n  \"end\": 56,\n  \"answer\": \"rainbows\"\n}\nIt drastically improves the outcome. This shows that we can’t use a “generic” string template which will generalise well across all contexts.\nLearning to Prompt\nLike many LLM applications the easiest way to generate an appropriate prompt is probably to use OpenAI or Mistral (instruct) to provide some templates. These (much) larger models can help us create ideal prompts (or even multiple prompts) which will enable us to select the best output from a QA model.\nSample prompt to both ChatGPT 3.5 and Mistral-Instruct:\nUser\nGiven a model schema for:\n\n- `summary_length`\n- `topic`\n\nGive me prompts that will allow me to extract those two fields\nChatGPT output:\n\nSure, here are some prompts you can use to extract the fields summary_length and topic:\nFor summary_length: 1. “Could you please provide the summary length?” 2. “What is the length of the summary?” 3. “How many words/characters is the summary?” 4. “Can you tell me the length of the summary in terms of word count/character count?” 5. “What’s the summarized length?”\nFor topic: 1. “What is the topic of the model?” 2. “Could you describe the main topic or subject matter?” 3. “What is the primary focus or theme?” 4. “What is the model primarily about?” 5. “Can you provide the topic or subject of the model?”\nThese prompts should help you extract the summary_length and topic from the model schema effectively.\n\nMistral-Instruct 7b Output\n\n\n“Please provide a brief summary of the article or text, keeping it under X words in length.” (where X is the maximum value of summary_length)\n“What is the main topic or theme of this text?”\n“Could you summarize the key points of the article or text in a concise manner, ensuring it does not exceed Y characters in total?” (where Y is &gt; the character limit equivalent of summary_length)\n“Kindly identify and write down the primary subject matter or category of the given text.”\n“Can you create a short abstract or synopsis of the provided document while adhering to our requirement of maintaining a word count below Z?” &gt; (where Z is the number of words equal to summary_length)\n“Which major idea or issue does this text primarily focus on? Please keep your response succinct.”\n\n\nWhen trying them through the QA model, the results are much more reasonable. These can be used as a starting point for creating a natural language UI with structured outputs, by defering the outcomes to a smaller, more specific model with constrained outputs.\nIn the scenario where we are using structured outputs, we would know the structure before at query time. This makes generating the appropriate prompts offline both feasible and a sensible approach to addressing this problem."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Building Natural Language UI using Extract Question Answering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nA Simple Feature Store\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nLLMs and lack of Interoperability\n\n\n\n\n\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\nJan 7, 2024\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nDiceless Table Top RPG Systems\n\n\n\n\n\n\n\nbob\n\n\ndiceless\n\n\nrpg\n\n\nllm\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nRevisting A Neural Probabilistic Model\n\n\n\n\n\n\n\ncode\n\n\npixi.js\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nColoring Sprites\n\n\n\n\n\n\n\ncode\n\n\npixi.js\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating CDFs using SymPy\n\n\nA look at Fire Emblems ‘True Hit’\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nInitial Thoughts on Prefect\n\n\n\n\n\n\n\nessay\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nA life well lived\n\n\n\n\n\n\n\nessay\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nGenerate Podcasts from a Script using Suno Bark!\n\n\n\n\n\n\n\ncode\n\n\nai\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\npixies\n\n\n\n\n\n\n  \n\n\n\n\nGentleBoost Algorithm\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\npixies\n\n\n\n\n\n\nNo matching items"
  }
]